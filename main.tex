\documentclass[runningheads,a4paper,11pt]{report}

\usepackage{algorithmic}
\usepackage{algorithm} 
\usepackage{array}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{comment} 
\usepackage{epsfig} 
\usepackage{fancyhdr}
\usepackage[T1]{fontenc}
\usepackage{geometry} 
\usepackage{graphicx}
\usepackage[colorlinks]{hyperref} 
\usepackage[latin1]{inputenc}
\usepackage{multicol}
\usepackage{multirow} 
\usepackage{rotating}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{url}
\usepackage{verbatim}
\usepackage{xcolor}
\usepackage{titlesec}

\geometry{a4paper,top=3cm,left=2cm,right=2cm,bottom=3cm}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{Project Popeye}
\fancyfoot[RE,LO]{MIRPR 2021-2022}
\fancyfoot[LE,RO]{\thepage}

\renewcommand{\headrulewidth}{2pt}
\renewcommand{\footrulewidth}{1pt}
\renewcommand{\headrule}{\hbox to\headwidth{%
  \color{lime}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrule}{\hbox to\headwidth{%
  \color{lime}\leaders\hrule height \footrulewidth\hfill}}
\renewcommand\bibname{References}
\hypersetup{
pdftitle={artTitle},
pdfauthor={name},
pdfkeywords={pdf, latex, tex, ps2pdf, dvipdfm, pdflatex},
bookmarksnumbered,
pdfstartview={FitH},
urlcolor=cyan,
colorlinks=true,
linkcolor=red,
citecolor=green,
}

\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

\linespread{1}

\makeindex


\begin{document}

\begin{titlepage}
\sloppy

\begin{center}
BABE\c S BOLYAI UNIVERSITY, CLUJ NAPOCA, ROM\^ ANIA

FACULTY OF MATHEMATICS AND COMPUTER SCIENCE

\vspace{6cm}

\Huge \textbf{Trying to master the game of chess with help of neural networks}

\vspace{1cm}

\normalsize -- MIRPR report --

\end{center}


\vspace{5cm}

\begin{flushright}
\Large{\textbf{Team members}}\\
Tudor Petru Cheregi, tudor.cheregi@stud.ubbcluj.ro\\
Victor Mihai Macinic, victor.macinic@stud.ubbcluj.ro
\end{flushright}

\vspace{4cm}

\begin{center}
2021-2022
\end{center}

\end{titlepage}

\pagenumbering{gobble}

\begin{abstract}
	    Throughout chess engines history, there have been proposed lots of different strategies in order to come up with the best algorithm possible. In comparison with very complex and general models that are capable of adapting to various environments (games), our approach is to create a neural network that is not capable of playing chess by itself, but rather is very good at evaluating random chess positions.
	    Since there exist known metrics for evaluating a given position/configuration of pieces
	    we can create our model on a fully supervised manner.
		The data needed to train the model is comprised of random chess positions from different games and different ELO ranges which can be found in the Big Chess DataBase, and their evaluation was obtained with the help of Stockfish (one of the best chess engines available for free).
\end{abstract}


\tableofcontents

\newpage

\newpage

\setstretch{1.5}



\newpage

\pagenumbering{arabic}


 


\chapter{Introduction}
\label{chapter:introduction}


Chess is an abstract strategy game and involves no hidden information. It is played on a square chessboard with 64 squares arranged in an eight-by-eight grid. At the start, each player (one controlling the white pieces, the other controlling the black pieces) controls sixteen pieces: one king, one queen, two rooks, two knights, two bishops, and eight pawns. The object of the game is to checkmate the opponent's king, whereby the king is under immediate attack (in "check") and there is no way for it to escape. There are also several ways a game can end in a draw.

One of the goals of early computer scientists was to create a chess-playing machine. In 1997, Deep Blue became the first computer to beat the reigning World Champion in a match when it defeated Garry Kasparov. Though not flawless, today's chess engines are significantly stronger than even the best human players, and have deeply influenced the development of chess theory.

In comparison with Deep Blue's strategy which was comprised of several heuristic methods and a minimax tree search based algorithm, our approach is a little bit closer to what modern engines (Stockfish NNUE, Alpha-Zero) make us of: neural networks.



\chapter{Scientific Problem}
\label{section:scientificProblem}


As we have mentioned before the game of chess is quite complex both in its rules and in its strategy and it is a pretty difficult task to take on with a conventional algorithm that simply uses a set of defined rules according to the current position. The reason of that is primarily due to our limited knowledge of how chess should be played. In essence the engine's level would be gaped to the potential of that set of rules. Thus, if we want to overcome this threshold and truly to achieve a breakthrough, deep learning would be our best chance.
Give a description of the problem.




\chapter{State of the art}
\label{chapter:stateOfArt}


In 2017, a revolutionary strategy was introduced by people from DeepMind who manage to create an algorithm that is able to generally play and adapt to any sort of game, given the necessary set of rules. Their approach was completely based on a deep reinforcement learning model.

Differences between DeepMind's approach and ours:
\begin{itemize}
	 \item Instead of using a reinforcement learning model, ours is based on a fully supervised method.
	 \item Our model is not able to cope with any different environment besides the game of chess since it was specifically trained for this task. 
\end{itemize}

\chapter{Approach}

    \section{Regression vs Classification}
    \textbf{Regression}: We first tried to mimic Stockfish's behaviour in terms of evaluating a position, therefore our models are supposed to predict a real value representing the exact evaluation of the current position.
    \\*\textbf{Classification}: Due to the complexity of the regression problem we tried to simplify the task and express the regression problem as a multi-label classification one. Thus, the evaluation of the position (the label) was converted into 3 classes based on which color is supposed to have a better position \\*(0 = black is better, 1 = equal position, 2 = white is better). 


    \section{Preprocessing}
    
    Our experiments have been conducted using three main tensor input representations, the first and the last one being inspired by \textbf{Matthia Sabatelli et al, 2018} \cite{paper1}.
    \\*\textbf{Algebric representation} - An 8x8 matrix, each square having its piece value (scaled to [-1,1] ) or 0 if there is no piece on the square.
    \\*\textbf{Sparse representation} - An 6x8x8 tensor representing an 8x8 matrix for every piece (Rook,Knight,Bishop,\\*Queen,King,Pawn), each square having a value of -1 (if there is a black piece on the square), 0 (if there is no piece on the square) or 1(if there is a white piece on the square).
    \\*\textbf{Bitmap representation} - An 12x8x8 tensor representing an 8x8 matrix for every piece (Rook,Knight,Bishop,\\*Queen,King,Pawn) of each color (Black or White), each square having a value of 0 (if there is no piece on the square) or 1(if there is a piece of the corresponding color on the square).
    \section{Models}
    As we have mentioned before, our goal is to create a decent evaluator for a given chess position rather than a model that is actually able to make a move. \\*We selected a large dataset of chess games from numerous grandmasters. Each game is comprised of a particular sequence of moves(positions) that we labeled using one of the strongest chess engines at the moment (Stockfish). Every position is afterwards turned into a FEN string representation and saved to our train data file.
    \\*Our experiments have been conducted using 2 main architectures based on exactly what has been proposed by \textbf{Matthia Sabatelli et al, 2018} \cite{paper1}: 
    \\*\textbf{Model 1} A network with fully connected layers
    that has an input layer of dimension width x height x channels for each tensor representation, 6 hidden layers( having 64,64,32,32,16,16 neurons), and one output layer with either 1 neuron for the regression task or 3 neurons for the classification one.
    \\*\textbf{Model 2} An FCN (fully convolutional network) to which have been added some fully connected layers in order to provide additional computation in hope of achieving better performance. The architecture is similar to the VGG architecture and it's output layer is the same as the one in the first model. 

\clearpage
\begin{algorithm}
	\caption{Training}
	\label{BestMove}
		\begin{algorithmic}
        
        \STATE \textbf{INPUT}
        \STATE B - data batches
        \STATE X - input data
        \STATE Y - labels for input data
        \STATE f - the network with parameters $\theta$
        \STATE \textbf{BEGIN}
        \FOR{i=1 TO numberOfEpochs}
            \FOR{j = 1 TO numberOfBatches}
                \STATE b $\leftarrow$ B[j];
                \STATE XBatch $\leftarrow$ all X from batch b;
                \STATE YBatch $\leftarrow$ all Y from batch b;
                \STATE predictions $\leftarrow$ f(XBatch, $\theta$);
                \STATE loss $\leftarrow$ MSE(predictions, YBatch);
                \STATE update $\theta$ with the obtained loss using an optimer, e.g. Adam
            \ENDFOR
        \ENDFOR
        \RETURN $\theta$
  		\STATE \textbf{END}
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
	\caption{Choose best move}
	\label{BestMove}
		\begin{algorithmic}
        
        \STATE \textbf{INPUT}
        \STATE  network: The neural network responsible for evaluating the                       position
        \STATE currentPosition: the current position represented as a FEN string
        
        \STATE \textbf{OUTPUT}
        \STATE  bestFen: The FEN representation of the next best move 
        
		\STATE \textbf{BEGIN}
  		\STATE nextMoves $\leftarrow$ getAllNextMoves(currentPosition);
  		\STATE bestFen $\leftarrow$ \textbf{NULL};
  		\STATE bestEval $\leftarrow$ \textbf{NULL};
  		\FOR{i=1 TO nextMoves.size()}
  		    \IF{bestFen = \textbf{NULL}}
  		        \STATE bestFen $\leftarrow$ nextMoves[i];
  		        \STATE bestEval $\leftarrow$ network.predict(bestFen);
  		    \ELSIF{network.predict(nextMoves[i]) > bestEval}
  		        \STATE bestFen $\leftarrow$ nextMoves[i];
  		        \STATE bestEval $\leftarrow$ network.predict(bestFen);
  		    \ENDIF
  		\ENDFOR
  		\RETURN bestFen;
  		\STATE \textbf{END}
\end{algorithmic}
\end{algorithm}

\chapter{Results}


\begin{thebibliography}{3}

\bibitem{paper1}
Matthia Sabatelli, Francesco Bidoia, Valeriu Codreanu and Marco Wiering
\\*Learning to Evaluate Chess Positions with Deep Neural Networks and Limited Lookahead

\bibitem{paper2}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot,
Laurent Sifre, Dharshan Kumaran, Thore Graepel,
Timothy Lillicrap, Karen Simonyan, Demis Hassabis
\\*Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm

\bibitem{paper3}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, 
Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, 
Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, 
David Silver
\\*Mastering Atari, Go, Chess and Shogi by Planning with a
Learned Model

\end{thebibliography}

\end{document}